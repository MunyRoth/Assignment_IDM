{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T19:42:53.657492Z",
     "start_time": "2024-01-09T19:42:53.343110Z"
    }
   },
   "id": "a402d1ee99a3458d",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Reading the dataset\n",
    "src_file = 'dataset.csv'\n",
    "dataframe = pd.read_csv(src_file, encoding=\"utf8\", quotechar=\"\\\"\", engine='python', usecols=[\"TITLE\", \"CATEGORY\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T19:42:53.700633Z",
     "start_time": "2024-01-09T19:42:53.660304Z"
    }
   },
   "id": "3b1829baa479ec81",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "101f20227e9d950d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cleaning Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b420ce0a1067a80"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Data\n",
      "\n",
      "TITLE         0\n",
      "CATEGORY    141\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking for missing data\n",
    "if any(dataframe.isnull().any()):\n",
    "    print('Missing Data\\n')\n",
    "    print(dataframe.isnull().sum())\n",
    "else:\n",
    "    print('No missing data')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T19:42:53.707280Z",
     "start_time": "2024-01-09T19:42:53.703246Z"
    }
   },
   "id": "5161cbbb7f4cbd7e",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows found\n",
      "Number of duplicate rows=  113\n",
      "Dropping duplicates\n",
      "\n",
      "(11823, 2)\n"
     ]
    }
   ],
   "source": [
    "# Checking for duplicate rows\n",
    "if any(dataframe.duplicated()):\n",
    "    print('Duplicate rows found')\n",
    "    print('Number of duplicate rows= ', dataframe[dataframe.duplicated()].shape[0])\n",
    "    dataframe.drop_duplicates(inplace=True, keep='first')\n",
    "    dataframe.reset_index(inplace=True, drop=True)\n",
    "    print('Dropping duplicates\\n')\n",
    "    print(dataframe.shape)\n",
    "else:\n",
    "    print('No duplicate data')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T19:42:53.721016Z",
     "start_time": "2024-01-09T19:42:53.708269Z"
    }
   },
   "id": "a4339de7d053796d",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing Pipeline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b294cc5c27045f2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "from sklearn import set_config\n",
    "\n",
    "# Set sklearn config to output Pandas DataFrame\n",
    "set_config(transform_output=\"pandas\")\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Function for cleaning and tokenize the headline\n",
    "def tokenize(doc):\n",
    "    document = doc.lower()  # convert the content of the headline to lowercase\n",
    "    document = re.sub(r'\\d+', '', document)  # remove all the digits inside the content (using regular expressions)\n",
    "    document = document.translate(str.maketrans('', '', string.punctuation))  # remove the punctuations (, . ! # ...)\n",
    "    document = document.strip()  # remove the spaces at the start and end of the headline\n",
    "    return [wnl.lemmatize(token) for token in word_tokenize(document) if token not in stopwords.words('english')]\n",
    "    # tokenize the headlines\n",
    "    # and then filter only the words that are not in the english stopwords (words that are commonly used and give no benefits to the classifier)\n",
    "    # and finally templatize all the tokens\n",
    "\n",
    "\n",
    "# Preprocessing Pipeline\n",
    "preprocessor = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(tokenizer=tokenize, token_pattern=None)),\n",
    "])\n",
    "\n",
    "# Transforming the dataset using TF-IDF\n",
    "tfidf_dataset = preprocessor.fit_transform(dataframe[\"TITLE\"].values)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T19:43:02.528390Z",
     "start_time": "2024-01-09T19:42:53.720711Z"
    }
   },
   "id": "77130c7432b96181",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ecef23e2d462e93"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Label encoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "489eb180e3756646"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "class_label = le.fit_transform(dataframe[\"CATEGORY\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T19:43:02.533040Z",
     "start_time": "2024-01-09T19:43:02.529771Z"
    }
   },
   "id": "bcd889de4730a32b",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train-Test Split"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10e2b2b138ac60e0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tfidf_dataset.toarray(),\n",
    "    class_label,\n",
    "    test_size=0.3  # the size of the testing dataset (in percentage between 0 and 1)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T19:43:03.316596Z",
     "start_time": "2024-01-09T19:43:02.533046Z"
    }
   },
   "id": "16930837d8390be6",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decision Tree Classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e683791d3dea8014"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of Decision Tree: 0.3831406822667043\n",
      "Precision score of Decision Tree: 0.386602608018235\n",
      "Recall score of Decision Tree: 0.3831406822667043\n",
      "F1 score of Decision Tree: 0.40764947400367807\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "DTClass = DecisionTreeClassifier(criterion=\"gini\", splitter=\"best\", random_state=42)\n",
    "\n",
    "# Train the classifier on the training dataset\n",
    "DTClass.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_dt = DTClass.predict(X_test)\n",
    "\n",
    "# Evaluating Decision Tree Model\n",
    "print(\"Accuracy score of Decision Tree:\", accuracy_score(y_test, y_pred_dt))\n",
    "print(\"Precision score of Decision Tree:\", precision_score(y_test, y_pred_dt, average='weighted', zero_division=1))\n",
    "print(\"Recall score of Decision Tree:\", recall_score(y_test, y_pred_dt, average='weighted', zero_division=1))\n",
    "print(\"F1 score of Decision Tree:\", f1_score(y_test, y_pred_dt, average='weighted', zero_division=1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T19:43:40.843200Z",
     "start_time": "2024-01-09T19:43:03.317486Z"
    }
   },
   "id": "5b3f204b09e5164f",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      1.00         2\n",
      "           1       1.00      0.00      0.00         3\n",
      "           2       1.00      0.00      0.00         2\n",
      "           3       0.28      0.16      0.21        43\n",
      "           4       0.52      0.71      0.60        17\n",
      "           5       1.00      0.00      0.00         2\n",
      "           6       1.00      0.12      0.22         8\n",
      "           7       1.00      0.00      0.00         1\n",
      "           8       0.00      0.00      1.00         6\n",
      "           9       0.00      0.00      1.00         2\n",
      "          10       0.50      0.67      0.57         3\n",
      "          11       1.00      0.00      0.00         1\n",
      "          12       0.50      0.45      0.48        11\n",
      "          13       0.41      0.32      0.36        63\n",
      "          14       0.45      0.50      0.48        10\n",
      "          15       0.00      0.00      1.00         2\n",
      "          16       0.00      1.00      0.00         0\n",
      "          17       1.00      0.00      0.00         1\n",
      "          18       1.00      0.00      0.00         1\n",
      "          20       1.00      0.00      0.00         1\n",
      "          23       0.15      0.26      0.19        46\n",
      "          24       1.00      0.00      0.00         1\n",
      "          25       0.00      0.00      1.00         1\n",
      "          26       0.00      0.00      1.00         4\n",
      "          27       0.00      0.00      1.00         7\n",
      "          29       0.00      0.00      1.00         6\n",
      "          30       1.00      0.00      0.00         2\n",
      "          31       1.00      0.00      0.00         2\n",
      "          32       1.00      0.00      0.00         1\n",
      "          33       0.38      0.30      0.33        40\n",
      "          35       1.00      0.00      0.00         4\n",
      "          36       0.44      0.42      0.43       136\n",
      "          37       0.00      0.00      1.00        20\n",
      "          38       0.63      0.70      0.66       191\n",
      "          39       1.00      0.00      0.00         4\n",
      "          40       1.00      0.00      0.00         1\n",
      "          41       0.00      0.00      1.00        15\n",
      "          42       0.00      0.00      1.00         5\n",
      "          43       0.18      0.11      0.13        28\n",
      "          44       0.16      0.14      0.15        22\n",
      "          45       0.21      0.14      0.17       127\n",
      "          47       1.00      0.00      0.00         3\n",
      "          48       0.20      0.08      0.11        13\n",
      "          49       0.00      0.00      1.00         5\n",
      "          50       0.50      0.50      0.50         6\n",
      "          51       0.00      1.00      0.00         0\n",
      "          52       0.22      0.33      0.27         6\n",
      "          55       0.33      0.25      0.29         4\n",
      "          56       0.38      0.31      0.34        36\n",
      "          58       1.00      0.33      0.50         3\n",
      "          59       0.12      0.10      0.11        20\n",
      "          60       0.00      0.00      1.00         2\n",
      "          61       0.28      0.21      0.24        33\n",
      "          62       0.00      1.00      0.00         0\n",
      "          63       1.00      0.00      0.00         1\n",
      "          64       0.00      0.00      1.00        10\n",
      "          65       1.00      0.00      0.00         1\n",
      "          66       1.00      0.00      0.00         1\n",
      "          67       1.00      0.00      0.00         4\n",
      "          69       1.00      0.00      0.00         1\n",
      "          70       0.06      0.10      0.08        20\n",
      "          71       1.00      0.17      0.29         6\n",
      "          72       1.00      0.00      0.00         2\n",
      "          73       1.00      0.00      0.00         2\n",
      "          74       0.00      1.00      0.00         0\n",
      "          75       0.50      0.33      0.40         3\n",
      "          76       0.00      1.00      0.00         0\n",
      "          77       0.44      0.80      0.57         5\n",
      "          79       0.18      0.11      0.13        56\n",
      "          80       0.33      0.25      0.29         4\n",
      "          81       1.00      0.00      0.00         1\n",
      "          82       1.00      0.00      0.00         1\n",
      "          83       1.00      0.50      0.67         4\n",
      "          84       0.10      0.06      0.07        18\n",
      "          86       0.24      0.31      0.27        35\n",
      "          88       1.00      0.00      0.00         2\n",
      "          89       0.12      0.10      0.11        10\n",
      "          90       1.00      0.00      0.00         1\n",
      "          91       1.00      0.00      0.00         4\n",
      "          92       0.24      0.23      0.23        22\n",
      "          93       0.14      0.10      0.12        10\n",
      "          94       1.00      0.00      0.00         2\n",
      "          95       1.00      1.00      1.00         1\n",
      "          96       1.00      0.00      0.00         3\n",
      "          97       1.00      0.00      0.00         2\n",
      "          98       1.00      0.00      0.00         2\n",
      "          99       0.09      0.08      0.09        24\n",
      "         100       0.10      0.07      0.08        15\n",
      "         101       1.00      0.00      0.00         1\n",
      "         102       0.20      1.00      0.33         1\n",
      "         103       1.00      0.00      0.00         2\n",
      "         104       0.33      0.50      0.40         2\n",
      "         106       0.34      0.29      0.32        78\n",
      "         108       0.00      0.00      1.00         1\n",
      "         109       1.00      0.00      0.00         2\n",
      "         110       1.00      0.00      0.00         4\n",
      "         111       1.00      0.00      0.00         1\n",
      "         112       0.00      1.00      0.00         0\n",
      "         113       0.14      0.14      0.14        22\n",
      "         114       0.00      0.00      1.00         1\n",
      "         115       1.00      0.00      0.00         1\n",
      "         117       0.00      0.00      1.00         2\n",
      "         118       0.00      0.00      1.00         3\n",
      "         119       1.00      0.00      0.00         3\n",
      "         120       0.50      0.29      0.36         7\n",
      "         121       1.00      1.00      1.00         2\n",
      "         123       0.80      0.50      0.62         8\n",
      "         124       1.00      0.60      0.75         5\n",
      "         125       0.83      0.62      0.71         8\n",
      "         126       1.00      0.00      0.00         1\n",
      "         127       1.00      0.00      0.00         7\n",
      "         128       0.14      0.05      0.08        19\n",
      "         129       0.00      0.00      1.00        11\n",
      "         130       0.40      0.25      0.31         8\n",
      "         131       0.23      0.31      0.27        62\n",
      "         132       0.89      0.94      0.91        17\n",
      "         133       0.20      0.07      0.11        27\n",
      "         134       0.00      1.00      0.00         0\n",
      "         135       1.00      0.00      0.00         1\n",
      "         136       0.00      0.00      1.00        11\n",
      "         137       1.00      1.00      1.00         1\n",
      "         138       0.47      0.44      0.46        18\n",
      "         141       1.00      0.00      0.00         1\n",
      "         144       1.00      0.00      0.00         4\n",
      "         145       0.00      0.00      1.00         3\n",
      "         146       0.00      1.00      0.00         0\n",
      "         147       0.37      0.35      0.36        57\n",
      "         148       0.33      0.11      0.17        27\n",
      "         149       0.05      0.05      0.05        22\n",
      "         150       0.00      0.00      1.00         7\n",
      "         151       0.00      0.00      1.00         1\n",
      "         152       1.00      0.00      0.00         1\n",
      "         153       0.00      1.00      0.00         0\n",
      "         154       1.00      0.00      0.00         1\n",
      "         155       0.00      0.00      1.00         2\n",
      "         156       0.00      1.00      0.00         0\n",
      "         157       0.15      0.17      0.16        36\n",
      "         158       0.12      0.13      0.13       119\n",
      "         159       0.06      0.04      0.04        28\n",
      "         161       0.71      0.83      0.77         6\n",
      "         163       0.00      0.00      1.00        12\n",
      "         164       0.00      0.00      1.00         1\n",
      "         165       1.00      0.00      0.00         3\n",
      "         166       0.00      0.00      1.00        11\n",
      "         168       1.00      0.50      0.67         2\n",
      "         169       0.14      0.15      0.15        13\n",
      "         170       0.25      0.20      0.22        25\n",
      "         171       0.00      1.00      0.00         0\n",
      "         172       1.00      0.00      0.00         4\n",
      "         174       0.50      0.40      0.44        10\n",
      "         176       1.00      0.00      0.00         2\n",
      "         177       0.00      0.00      1.00         3\n",
      "         178       0.00      0.00      1.00         4\n",
      "         179       0.25      0.20      0.22         5\n",
      "         180       0.36      0.29      0.32        14\n",
      "         181       0.26      0.16      0.20        31\n",
      "         182       0.60      0.60      0.60         5\n",
      "         183       0.52      0.50      0.51        32\n",
      "         184       1.00      0.00      0.00         1\n",
      "         185       0.00      0.00      1.00         4\n",
      "         186       1.00      0.00      0.00         6\n",
      "         187       0.14      0.11      0.12        47\n",
      "         188       1.00      0.00      0.00         1\n",
      "         189       0.00      0.00      1.00         3\n",
      "         190       1.00      1.00      1.00         1\n",
      "         191       1.00      0.00      0.00         4\n",
      "         192       0.14      0.11      0.12         9\n",
      "         193       0.36      0.50      0.42        20\n",
      "         194       1.00      0.00      0.00         2\n",
      "         195       0.56      0.56      0.56         9\n",
      "         196       0.49      0.69      0.57       816\n",
      "         198       1.00      0.00      0.00         2\n",
      "         199       1.00      0.00      0.00         1\n",
      "         201       0.00      0.00      1.00         1\n",
      "         202       1.00      0.00      0.00         1\n",
      "         204       0.73      0.66      0.70        71\n",
      "         205       0.33      0.27      0.30        33\n",
      "         207       0.00      0.00      1.00         4\n",
      "         208       1.00      0.67      0.80         3\n",
      "         209       0.19      0.25      0.21        12\n",
      "         210       1.00      1.00      1.00         1\n",
      "         211       0.33      0.42      0.37        69\n",
      "         212       1.00      0.00      0.00         2\n",
      "         214       0.45      0.44      0.44       304\n",
      "         215       0.11      0.07      0.08        45\n",
      "\n",
      "    accuracy                           0.38      3547\n",
      "   macro avg       0.50      0.23      0.35      3547\n",
      "weighted avg       0.39      0.38      0.41      3547\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_dt, zero_division=1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T19:43:40.855418Z",
     "start_time": "2024-01-09T19:43:40.843338Z"
    }
   },
   "id": "1cd72a4ebf0320a8",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes Classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f89306fce659115c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of Naive Bayes: 0.28813081477304764\n",
      "Precision score of Naive Bayes: 0.7179151969708233\n",
      "Recall score of Naive Bayes: 0.28813081477304764\n",
      "F1 score of Naive Bayes: 0.20320948016608492\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create a Multinomial Naive Bayes classifier\n",
    "NBClass = MultinomialNB()\n",
    "\n",
    "# Train the classifier on the training dataset\n",
    "NBClass.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_nb = NBClass.predict(X_test)\n",
    "\n",
    "# Evaluating Naive Bayes Model\n",
    "print(\"Accuracy score of Naive Bayes:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(\"Precision score of Naive Bayes:\", precision_score(y_test, y_pred_nb, average='weighted', zero_division=1))\n",
    "print(\"Recall score of Naive Bayes:\", recall_score(y_test, y_pred_nb, average='weighted', zero_division=1))\n",
    "print(\"F1 score of Naive Bayes:\", f1_score(y_test, y_pred_nb, average='weighted', zero_division=1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T19:43:41.976219Z",
     "start_time": "2024-01-09T19:43:40.856548Z"
    }
   },
   "id": "8ac9a14fbe75bc7",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00         2\n",
      "           1       1.00      0.00      0.00         3\n",
      "           2       1.00      0.00      0.00         2\n",
      "           3       1.00      0.00      0.00        43\n",
      "           4       1.00      0.00      0.00        17\n",
      "           5       1.00      0.00      0.00         2\n",
      "           6       1.00      0.00      0.00         8\n",
      "           7       1.00      0.00      0.00         1\n",
      "           8       1.00      0.00      0.00         6\n",
      "           9       1.00      0.00      0.00         2\n",
      "          10       1.00      0.00      0.00         3\n",
      "          11       1.00      0.00      0.00         1\n",
      "          12       1.00      0.00      0.00        11\n",
      "          13       1.00      0.00      0.00        63\n",
      "          14       1.00      0.00      0.00        10\n",
      "          15       1.00      0.00      0.00         2\n",
      "          17       1.00      0.00      0.00         1\n",
      "          18       1.00      0.00      0.00         1\n",
      "          20       1.00      0.00      0.00         1\n",
      "          23       1.00      0.00      0.00        46\n",
      "          24       1.00      0.00      0.00         1\n",
      "          25       1.00      0.00      0.00         1\n",
      "          26       1.00      0.00      0.00         4\n",
      "          27       1.00      0.00      0.00         7\n",
      "          29       1.00      0.00      0.00         6\n",
      "          30       1.00      0.00      0.00         2\n",
      "          31       1.00      0.00      0.00         2\n",
      "          32       1.00      0.00      0.00         1\n",
      "          33       1.00      0.00      0.00        40\n",
      "          35       1.00      0.00      0.00         4\n",
      "          36       0.62      0.27      0.38       136\n",
      "          37       1.00      0.00      0.00        20\n",
      "          38       0.68      0.28      0.39       191\n",
      "          39       1.00      0.00      0.00         4\n",
      "          40       1.00      0.00      0.00         1\n",
      "          41       1.00      0.00      0.00        15\n",
      "          42       1.00      0.00      0.00         5\n",
      "          43       1.00      0.00      0.00        28\n",
      "          44       1.00      0.00      0.00        22\n",
      "          45       0.00      0.00      1.00       127\n",
      "          47       1.00      0.00      0.00         3\n",
      "          48       1.00      0.00      0.00        13\n",
      "          49       1.00      0.00      0.00         5\n",
      "          50       1.00      0.00      0.00         6\n",
      "          52       1.00      0.00      0.00         6\n",
      "          55       1.00      0.00      0.00         4\n",
      "          56       1.00      0.00      0.00        36\n",
      "          58       1.00      0.00      0.00         3\n",
      "          59       1.00      0.00      0.00        20\n",
      "          60       1.00      0.00      0.00         2\n",
      "          61       1.00      0.00      0.00        33\n",
      "          63       1.00      0.00      0.00         1\n",
      "          64       1.00      0.00      0.00        10\n",
      "          65       1.00      0.00      0.00         1\n",
      "          66       1.00      0.00      0.00         1\n",
      "          67       1.00      0.00      0.00         4\n",
      "          69       1.00      0.00      0.00         1\n",
      "          70       1.00      0.00      0.00        20\n",
      "          71       1.00      0.00      0.00         6\n",
      "          72       1.00      0.00      0.00         2\n",
      "          73       1.00      0.00      0.00         2\n",
      "          75       1.00      0.00      0.00         3\n",
      "          77       1.00      0.00      0.00         5\n",
      "          79       1.00      0.00      0.00        56\n",
      "          80       1.00      0.00      0.00         4\n",
      "          81       1.00      0.00      0.00         1\n",
      "          82       1.00      0.00      0.00         1\n",
      "          83       1.00      0.00      0.00         4\n",
      "          84       1.00      0.00      0.00        18\n",
      "          86       1.00      0.00      0.00        35\n",
      "          88       1.00      0.00      0.00         2\n",
      "          89       1.00      0.00      0.00        10\n",
      "          90       1.00      0.00      0.00         1\n",
      "          91       1.00      0.00      0.00         4\n",
      "          92       1.00      0.00      0.00        22\n",
      "          93       1.00      0.00      0.00        10\n",
      "          94       1.00      0.00      0.00         2\n",
      "          95       1.00      0.00      0.00         1\n",
      "          96       1.00      0.00      0.00         3\n",
      "          97       1.00      0.00      0.00         2\n",
      "          98       1.00      0.00      0.00         2\n",
      "          99       1.00      0.00      0.00        24\n",
      "         100       1.00      0.00      0.00        15\n",
      "         101       1.00      0.00      0.00         1\n",
      "         102       1.00      0.00      0.00         1\n",
      "         103       1.00      0.00      0.00         2\n",
      "         104       1.00      0.00      0.00         2\n",
      "         106       1.00      0.00      0.00        78\n",
      "         108       1.00      0.00      0.00         1\n",
      "         109       1.00      0.00      0.00         2\n",
      "         110       1.00      0.00      0.00         4\n",
      "         111       1.00      0.00      0.00         1\n",
      "         113       1.00      0.00      0.00        22\n",
      "         114       1.00      0.00      0.00         1\n",
      "         115       1.00      0.00      0.00         1\n",
      "         117       1.00      0.00      0.00         2\n",
      "         118       1.00      0.00      0.00         3\n",
      "         119       1.00      0.00      0.00         3\n",
      "         120       1.00      0.00      0.00         7\n",
      "         121       1.00      0.00      0.00         2\n",
      "         123       1.00      0.00      0.00         8\n",
      "         124       1.00      0.00      0.00         5\n",
      "         125       1.00      0.00      0.00         8\n",
      "         126       1.00      0.00      0.00         1\n",
      "         127       1.00      0.00      0.00         7\n",
      "         128       1.00      0.00      0.00        19\n",
      "         129       1.00      0.00      0.00        11\n",
      "         130       1.00      0.00      0.00         8\n",
      "         131       1.00      0.00      0.00        62\n",
      "         132       1.00      0.00      0.00        17\n",
      "         133       1.00      0.00      0.00        27\n",
      "         135       1.00      0.00      0.00         1\n",
      "         136       1.00      0.00      0.00        11\n",
      "         137       1.00      0.00      0.00         1\n",
      "         138       1.00      0.00      0.00        18\n",
      "         141       1.00      0.00      0.00         1\n",
      "         144       1.00      0.00      0.00         4\n",
      "         145       1.00      0.00      0.00         3\n",
      "         147       1.00      0.00      0.00        57\n",
      "         148       1.00      0.00      0.00        27\n",
      "         149       1.00      0.00      0.00        22\n",
      "         150       1.00      0.00      0.00         7\n",
      "         151       1.00      0.00      0.00         1\n",
      "         152       1.00      0.00      0.00         1\n",
      "         154       1.00      0.00      0.00         1\n",
      "         155       1.00      0.00      0.00         2\n",
      "         157       1.00      0.00      0.00        36\n",
      "         158       1.00      0.00      0.00       119\n",
      "         159       1.00      0.00      0.00        28\n",
      "         161       1.00      0.00      0.00         6\n",
      "         163       1.00      0.00      0.00        12\n",
      "         164       1.00      0.00      0.00         1\n",
      "         165       1.00      0.00      0.00         3\n",
      "         166       1.00      0.00      0.00        11\n",
      "         168       1.00      0.00      0.00         2\n",
      "         169       1.00      0.00      0.00        13\n",
      "         170       1.00      0.00      0.00        25\n",
      "         172       1.00      0.00      0.00         4\n",
      "         174       1.00      0.00      0.00        10\n",
      "         176       1.00      0.00      0.00         2\n",
      "         177       1.00      0.00      0.00         3\n",
      "         178       1.00      0.00      0.00         4\n",
      "         179       1.00      0.00      0.00         5\n",
      "         180       1.00      0.00      0.00        14\n",
      "         181       1.00      0.00      0.00        31\n",
      "         182       1.00      0.00      0.00         5\n",
      "         183       1.00      0.00      0.00        32\n",
      "         184       1.00      0.00      0.00         1\n",
      "         185       1.00      0.00      0.00         4\n",
      "         186       1.00      0.00      0.00         6\n",
      "         187       1.00      0.00      0.00        47\n",
      "         188       1.00      0.00      0.00         1\n",
      "         189       1.00      0.00      0.00         3\n",
      "         190       1.00      0.00      0.00         1\n",
      "         191       1.00      0.00      0.00         4\n",
      "         192       1.00      0.00      0.00         9\n",
      "         193       1.00      0.00      0.00        20\n",
      "         194       1.00      0.00      0.00         2\n",
      "         195       1.00      0.00      0.00         9\n",
      "         196       0.26      0.99      0.41       816\n",
      "         198       1.00      0.00      0.00         2\n",
      "         199       1.00      0.00      0.00         1\n",
      "         201       1.00      0.00      0.00         1\n",
      "         202       1.00      0.00      0.00         1\n",
      "         204       1.00      0.00      0.00        71\n",
      "         205       1.00      0.00      0.00        33\n",
      "         207       1.00      0.00      0.00         4\n",
      "         208       1.00      0.00      0.00         3\n",
      "         209       1.00      0.00      0.00        12\n",
      "         210       1.00      0.00      0.00         1\n",
      "         211       1.00      0.00      0.00        69\n",
      "         212       1.00      0.00      0.00         2\n",
      "         214       0.50      0.40      0.44       304\n",
      "         215       1.00      0.00      0.00        45\n",
      "\n",
      "    accuracy                           0.29      3547\n",
      "   macro avg       0.98      0.01      0.02      3547\n",
      "weighted avg       0.72      0.29      0.20      3547\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_nb, zero_division=1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T19:43:41.991552Z",
     "start_time": "2024-01-09T19:43:41.973404Z"
    }
   },
   "id": "4dbe03180d1b21e2",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Network Classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc7d19e301eebc89"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/munyroth/Desktop/Python/Assignment/.venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "233/233 [==============================] - 1s 2ms/step - loss: 3.8135 - accuracy: 0.2240 - val_loss: 3.5301 - val_accuracy: 0.2174\n",
      "Epoch 2/20\n",
      "233/233 [==============================] - 0s 2ms/step - loss: 3.3063 - accuracy: 0.2292 - val_loss: 3.5082 - val_accuracy: 0.2379\n",
      "Epoch 3/20\n",
      "233/233 [==============================] - 0s 2ms/step - loss: 2.8513 - accuracy: 0.2998 - val_loss: 3.5729 - val_accuracy: 0.2826\n",
      "Epoch 4/20\n",
      "233/233 [==============================] - 0s 2ms/step - loss: 2.4041 - accuracy: 0.3759 - val_loss: 3.8910 - val_accuracy: 0.2464\n",
      "Epoch 5/20\n",
      "233/233 [==============================] - 1s 2ms/step - loss: 2.0130 - accuracy: 0.4513 - val_loss: 4.7252 - val_accuracy: 0.2452\n",
      "Epoch 6/20\n",
      "233/233 [==============================] - 0s 2ms/step - loss: 1.7400 - accuracy: 0.5073 - val_loss: 5.3311 - val_accuracy: 0.2307\n",
      "Epoch 7/20\n",
      "233/233 [==============================] - 1s 2ms/step - loss: 1.4980 - accuracy: 0.5662 - val_loss: 6.2863 - val_accuracy: 0.2234\n",
      "Epoch 8/20\n",
      "233/233 [==============================] - 1s 3ms/step - loss: 1.2865 - accuracy: 0.6147 - val_loss: 7.4014 - val_accuracy: 0.2101\n",
      "Epoch 9/20\n",
      "233/233 [==============================] - 0s 2ms/step - loss: 1.1282 - accuracy: 0.6639 - val_loss: 7.9330 - val_accuracy: 0.2114\n",
      "Epoch 10/20\n",
      "233/233 [==============================] - 1s 2ms/step - loss: 0.9949 - accuracy: 0.7027 - val_loss: 8.9586 - val_accuracy: 0.2258\n",
      "Epoch 11/20\n",
      "233/233 [==============================] - 0s 2ms/step - loss: 0.8620 - accuracy: 0.7465 - val_loss: 9.5358 - val_accuracy: 0.2283\n",
      "Epoch 12/20\n",
      "233/233 [==============================] - 1s 2ms/step - loss: 0.7257 - accuracy: 0.7840 - val_loss: 10.4378 - val_accuracy: 0.2077\n",
      "Epoch 13/20\n",
      "233/233 [==============================] - 0s 2ms/step - loss: 0.6420 - accuracy: 0.8111 - val_loss: 11.3027 - val_accuracy: 0.2295\n",
      "Epoch 14/20\n",
      "233/233 [==============================] - 0s 2ms/step - loss: 0.5522 - accuracy: 0.8371 - val_loss: 12.3850 - val_accuracy: 0.2126\n",
      "Epoch 15/20\n",
      "233/233 [==============================] - 0s 2ms/step - loss: 0.4714 - accuracy: 0.8613 - val_loss: 13.5103 - val_accuracy: 0.2174\n",
      "Epoch 16/20\n",
      "233/233 [==============================] - 0s 2ms/step - loss: 0.4361 - accuracy: 0.8720 - val_loss: 14.0002 - val_accuracy: 0.2246\n",
      "Epoch 17/20\n",
      "233/233 [==============================] - 0s 2ms/step - loss: 0.4107 - accuracy: 0.8839 - val_loss: 14.5087 - val_accuracy: 0.2343\n",
      "Epoch 18/20\n",
      "233/233 [==============================] - 0s 2ms/step - loss: 0.3557 - accuracy: 0.8961 - val_loss: 15.5264 - val_accuracy: 0.2150\n",
      "Epoch 19/20\n",
      "233/233 [==============================] - 0s 2ms/step - loss: 0.3224 - accuracy: 0.9045 - val_loss: 15.8744 - val_accuracy: 0.2258\n",
      "Epoch 20/20\n",
      "233/233 [==============================] - 1s 2ms/step - loss: 0.2902 - accuracy: 0.9119 - val_loss: 16.1827 - val_accuracy: 0.2234\n",
      "111/111 [==============================] - 0s 569us/step\n",
      "Accuracy score of Neural Network: 0.21962221595714687\n",
      "Precision score of Neural Network: 0.3374365345105932\n",
      "Recall score of Neural Network: 0.21962221595714687\n",
      "F1 score of Neural Network: 0.3772620767134382\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential()\n",
    "# Input layer\n",
    "model.add(Dense(16, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "# Hidden layers\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "# Output layer\n",
    "num_classes = len(np.unique(class_label))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_nn = np.argmax(model.predict(X_test), axis=-1)\n",
    "\n",
    "# Evaluating Neural Network Model\n",
    "print(\"Accuracy score of Neural Network:\", accuracy_score(y_test, y_pred_nn))\n",
    "print(\"Precision score of Neural Network:\", precision_score(y_test, y_pred_nn, average='weighted', zero_division=1))\n",
    "print(\"Recall score of Neural Network:\", recall_score(y_test, y_pred_nn, average='weighted', zero_division=1))\n",
    "print(\"F1 score of Neural Network:\", f1_score(y_test, y_pred_nn, average='weighted', zero_division=1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T19:44:19.187595Z",
     "start_time": "2024-01-09T19:43:41.989962Z"
    }
   },
   "id": "e4f1af841a349988",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      1.00         2\n",
      "           1       0.00      0.00      1.00         3\n",
      "           2       1.00      0.00      0.00         2\n",
      "           3       0.25      0.09      0.14        43\n",
      "           4       0.00      0.00      1.00        17\n",
      "           5       1.00      0.00      0.00         2\n",
      "           6       0.00      0.00      1.00         8\n",
      "           7       1.00      0.00      0.00         1\n",
      "           8       1.00      0.00      0.00         6\n",
      "           9       0.00      0.00      1.00         2\n",
      "          10       0.00      0.00      1.00         3\n",
      "          11       1.00      0.00      0.00         1\n",
      "          12       0.00      0.00      1.00        11\n",
      "          13       0.19      0.16      0.17        63\n",
      "          14       1.00      0.10      0.18        10\n",
      "          15       0.00      0.00      1.00         2\n",
      "          17       1.00      0.00      0.00         1\n",
      "          18       1.00      0.00      0.00         1\n",
      "          20       1.00      0.00      0.00         1\n",
      "          23       0.07      0.17      0.10        46\n",
      "          24       1.00      0.00      0.00         1\n",
      "          25       0.00      0.00      1.00         1\n",
      "          26       1.00      0.00      0.00         4\n",
      "          27       0.00      0.00      1.00         7\n",
      "          29       0.00      0.00      1.00         6\n",
      "          30       1.00      0.00      0.00         2\n",
      "          31       1.00      0.00      0.00         2\n",
      "          32       1.00      0.00      0.00         1\n",
      "          33       0.15      0.12      0.14        40\n",
      "          35       0.00      0.00      1.00         4\n",
      "          36       0.30      0.20      0.24       136\n",
      "          37       0.06      0.10      0.08        20\n",
      "          38       0.26      0.31      0.28       191\n",
      "          39       1.00      0.00      0.00         4\n",
      "          40       1.00      0.00      0.00         1\n",
      "          41       0.00      0.00      1.00        15\n",
      "          42       0.00      0.00      1.00         5\n",
      "          43       0.00      0.00      1.00        28\n",
      "          44       0.00      0.00      1.00        22\n",
      "          45       0.16      0.20      0.17       127\n",
      "          47       1.00      0.00      0.00         3\n",
      "          48       0.00      0.00      1.00        13\n",
      "          49       1.00      0.00      0.00         5\n",
      "          50       0.10      0.17      0.12         6\n",
      "          52       0.00      0.00      1.00         6\n",
      "          54       0.00      1.00      0.00         0\n",
      "          55       0.00      0.00      1.00         4\n",
      "          56       0.17      0.11      0.14        36\n",
      "          58       1.00      0.00      0.00         3\n",
      "          59       0.00      0.00      1.00        20\n",
      "          60       1.00      0.00      0.00         2\n",
      "          61       0.19      0.09      0.12        33\n",
      "          62       0.00      1.00      0.00         0\n",
      "          63       0.00      0.00      1.00         1\n",
      "          64       0.00      0.00      1.00        10\n",
      "          65       1.00      0.00      0.00         1\n",
      "          66       1.00      0.00      0.00         1\n",
      "          67       1.00      0.00      0.00         4\n",
      "          69       1.00      0.00      0.00         1\n",
      "          70       0.00      0.00      1.00        20\n",
      "          71       0.33      0.17      0.22         6\n",
      "          72       1.00      0.00      0.00         2\n",
      "          73       1.00      0.00      0.00         2\n",
      "          75       0.00      0.00      1.00         3\n",
      "          77       0.33      0.20      0.25         5\n",
      "          79       0.18      0.20      0.19        56\n",
      "          80       0.00      0.00      1.00         4\n",
      "          81       1.00      0.00      0.00         1\n",
      "          82       1.00      0.00      0.00         1\n",
      "          83       1.00      0.25      0.40         4\n",
      "          84       0.02      0.06      0.03        18\n",
      "          85       0.00      1.00      0.00         0\n",
      "          86       0.06      0.20      0.10        35\n",
      "          88       1.00      0.00      0.00         2\n",
      "          89       1.00      0.00      0.00        10\n",
      "          90       1.00      0.00      0.00         1\n",
      "          91       1.00      0.00      0.00         4\n",
      "          92       0.02      0.05      0.03        22\n",
      "          93       0.00      0.00      1.00        10\n",
      "          94       1.00      0.00      0.00         2\n",
      "          95       1.00      1.00      1.00         1\n",
      "          96       0.00      0.00      1.00         3\n",
      "          97       0.00      0.00      1.00         2\n",
      "          98       1.00      0.00      0.00         2\n",
      "          99       0.09      0.12      0.10        24\n",
      "         100       0.00      0.00      1.00        15\n",
      "         101       0.00      0.00      1.00         1\n",
      "         102       0.00      0.00      1.00         1\n",
      "         103       0.00      0.00      1.00         2\n",
      "         104       1.00      0.00      0.00         2\n",
      "         106       0.04      0.06      0.05        78\n",
      "         108       1.00      0.00      0.00         1\n",
      "         109       0.00      0.00      1.00         2\n",
      "         110       1.00      0.00      0.00         4\n",
      "         111       1.00      0.00      0.00         1\n",
      "         113       0.04      0.09      0.05        22\n",
      "         114       1.00      0.00      0.00         1\n",
      "         115       1.00      0.00      0.00         1\n",
      "         117       0.00      0.00      1.00         2\n",
      "         118       0.00      0.00      1.00         3\n",
      "         119       1.00      0.00      0.00         3\n",
      "         120       1.00      0.00      0.00         7\n",
      "         121       1.00      0.00      0.00         2\n",
      "         123       0.00      0.00      1.00         8\n",
      "         124       0.50      0.20      0.29         5\n",
      "         125       0.67      0.25      0.36         8\n",
      "         126       0.00      0.00      1.00         1\n",
      "         127       0.00      0.00      1.00         7\n",
      "         128       0.07      0.16      0.09        19\n",
      "         129       0.00      0.00      1.00        11\n",
      "         130       0.14      0.12      0.13         8\n",
      "         131       0.05      0.10      0.07        62\n",
      "         132       1.00      0.06      0.11        17\n",
      "         133       0.00      0.00      1.00        27\n",
      "         135       1.00      0.00      0.00         1\n",
      "         136       0.00      0.00      1.00        11\n",
      "         137       0.00      0.00      1.00         1\n",
      "         138       0.05      0.11      0.07        18\n",
      "         140       0.00      1.00      0.00         0\n",
      "         141       1.00      0.00      0.00         1\n",
      "         144       0.00      0.00      1.00         4\n",
      "         145       0.00      0.00      1.00         3\n",
      "         146       0.00      1.00      0.00         0\n",
      "         147       0.17      0.07      0.10        57\n",
      "         148       0.13      0.11      0.12        27\n",
      "         149       0.20      0.05      0.07        22\n",
      "         150       0.00      0.00      1.00         7\n",
      "         151       1.00      0.00      0.00         1\n",
      "         152       1.00      0.00      0.00         1\n",
      "         154       1.00      0.00      0.00         1\n",
      "         155       1.00      0.00      0.00         2\n",
      "         157       0.06      0.19      0.09        36\n",
      "         158       0.15      0.12      0.13       119\n",
      "         159       0.11      0.04      0.05        28\n",
      "         161       0.00      0.00      1.00         6\n",
      "         162       0.00      1.00      0.00         0\n",
      "         163       0.06      0.17      0.09        12\n",
      "         164       0.00      0.00      1.00         1\n",
      "         165       1.00      0.00      0.00         3\n",
      "         166       0.00      0.00      1.00        11\n",
      "         168       0.20      0.50      0.29         2\n",
      "         169       0.00      0.00      1.00        13\n",
      "         170       0.00      0.00      1.00        25\n",
      "         172       0.00      0.00      1.00         4\n",
      "         174       0.57      0.40      0.47        10\n",
      "         176       0.00      0.00      1.00         2\n",
      "         177       1.00      0.00      0.00         3\n",
      "         178       0.33      0.25      0.29         4\n",
      "         179       0.00      0.00      1.00         5\n",
      "         180       0.29      0.14      0.19        14\n",
      "         181       0.00      0.00      1.00        31\n",
      "         182       0.00      0.00      1.00         5\n",
      "         183       0.10      0.22      0.14        32\n",
      "         184       1.00      0.00      0.00         1\n",
      "         185       1.00      0.00      0.00         4\n",
      "         186       0.00      0.00      1.00         6\n",
      "         187       0.05      0.06      0.06        47\n",
      "         188       1.00      0.00      0.00         1\n",
      "         189       0.33      0.33      0.33         3\n",
      "         190       1.00      0.00      0.00         1\n",
      "         191       1.00      0.00      0.00         4\n",
      "         192       0.00      0.00      1.00         9\n",
      "         193       0.31      0.20      0.24        20\n",
      "         194       0.00      0.00      1.00         2\n",
      "         195       0.50      0.11      0.18         9\n",
      "         196       0.71      0.48      0.57       816\n",
      "         198       1.00      0.00      0.00         2\n",
      "         199       1.00      0.00      0.00         1\n",
      "         201       1.00      0.00      0.00         1\n",
      "         202       1.00      0.00      0.00         1\n",
      "         204       0.23      0.28      0.25        71\n",
      "         205       0.67      0.06      0.11        33\n",
      "         207       0.00      0.00      1.00         4\n",
      "         208       1.00      0.00      0.00         3\n",
      "         209       0.00      0.00      1.00        12\n",
      "         210       1.00      0.00      0.00         1\n",
      "         211       0.12      0.07      0.09        69\n",
      "         212       0.00      0.00      1.00         2\n",
      "         214       0.48      0.36      0.41       304\n",
      "         215       0.01      0.02      0.01        45\n",
      "\n",
      "    accuracy                           0.22      3547\n",
      "   macro avg       0.42      0.09      0.39      3547\n",
      "weighted avg       0.34      0.22      0.38      3547\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_nn, zero_division=1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T19:44:19.189280Z",
     "start_time": "2024-01-09T19:43:57.462733Z"
    }
   },
   "id": "d3c13a1b36a50e5f",
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
